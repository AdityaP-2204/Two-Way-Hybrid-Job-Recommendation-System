{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFnwYdVd6bSu",
        "outputId": "9c88765f-8eba-4a59-9484-2372c231957e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synthesizing 10000 users, 500 jobs, and 2000000 interactions...\n",
            "users.csv created.\n",
            "jobs.csv created.\n",
            "--- Part 3: Synthesizing Interactions ---\n",
            "Generating 1000000 positive samples...\n",
            "Generating 1000000 negative samples...\n",
            "Balancing and cleaning dataset for 40/60 split...\n",
            "Cleaned samples: 10000 positive, 904774 negative.\n",
            "Using all 10000 positive samples, downsampling negatives...\n",
            "interactions.csv created.\n",
            "Total interactions: 24999\n",
            "Positive interactions: 10000 (40.0%)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "# --- Configuration ---\n",
        "NUM_USERS = 10000\n",
        "NUM_JOBS = 500\n",
        "NUM_INTERACTIONS = 2000000  # ~1M positive, ~1M negative\n",
        "\n",
        "# Define skill columns\n",
        "SKILLS = ['C++', 'Python', 'Java', 'SQL', 'Excel', 'JavaScript', 'HTML', 'CSS', 'Machine Learning', 'Data Analysis', 'Project Management']\n",
        "\n",
        "print(f\"Synthesizing {NUM_USERS} users, {NUM_JOBS} jobs, and {NUM_INTERACTIONS} interactions...\")\n",
        "\n",
        "# --- 1. Synthesize users.csv ---\n",
        "user_data = {\n",
        "    'user_id': range(NUM_USERS),\n",
        "    'experience': np.random.randint(0, 25, size=NUM_USERS),\n",
        "    'salary_expectation': np.random.randint(40000, 150000, size=NUM_USERS)\n",
        "}\n",
        "# Generate random skill proficiencies (0-10)\n",
        "user_skills = np.random.rand(NUM_USERS, len(SKILLS)) * 10\n",
        "for i, skill in enumerate(SKILLS):\n",
        "    user_data[f'{skill}_proficiency'] = user_skills[:, i]\n",
        "\n",
        "users_df = pd.DataFrame(user_data)\n",
        "users_df.to_csv('users.csv', index=False)\n",
        "print(\"users.csv created.\")\n",
        "\n",
        "# --- 2. Synthesize jobs.csv ---\n",
        "job_data = {\n",
        "    'job_id': range(NUM_JOBS),\n",
        "    'experience_required': np.random.randint(0, 15, size=NUM_JOBS),\n",
        "}\n",
        "# Generate salary bands\n",
        "job_data['salary_min'] = np.random.randint(30000, 120000, size=NUM_JOBS)\n",
        "job_data['salary_max'] = job_data['salary_min'] + np.random.randint(10000, 50000, size=NUM_JOBS)\n",
        "\n",
        "# Generate skill expectations (0-10)\n",
        "job_skills = np.random.rand(NUM_JOBS, len(SKILLS)) * 10\n",
        "for i, skill in enumerate(SKILLS):\n",
        "    job_data[f'{skill}_expected'] = job_skills[:, i]\n",
        "\n",
        "jobs_df = pd.DataFrame(job_data)\n",
        "jobs_df.to_csv('jobs.csv', index=False)\n",
        "print(\"jobs.csv created.\")\n",
        "\n",
        "\n",
        "# --- 3. Synthesize interactions.csv (The \"Ground Truth\") ---\n",
        "print(\"--- Part 3: Synthesizing Interactions ---\")\n",
        "\n",
        "# Get skill matrices (assuming users_df and jobs_df exist)\n",
        "user_skill_matrix = users_df[[f'{s}_proficiency' for s in SKILLS]].values\n",
        "job_skill_matrix = jobs_df[[f'{s}_expected' for s in SKILLS]].values\n",
        "\n",
        "# Normalize for cosine similarity\n",
        "user_skill_matrix_norm = normalize(user_skill_matrix)\n",
        "job_skill_matrix_norm = normalize(job_skill_matrix)\n",
        "\n",
        "# We will generate positive and negative samples in separate lists\n",
        "interaction_data_pos = []\n",
        "interaction_data_neg = []\n",
        "\n",
        "# --- Generate positive samples (good matches) ---\n",
        "print(f\"Generating {NUM_INTERACTIONS // 2} positive samples...\")\n",
        "for _ in range(NUM_INTERACTIONS // 2):\n",
        "    user_id = np.random.randint(0, NUM_USERS)\n",
        "    user = users_df.iloc[user_id]\n",
        "    user_skills_vec = user_skill_matrix_norm[user_id]\n",
        "\n",
        "    skill_similarities = job_skill_matrix_norm.dot(user_skills_vec)\n",
        "    top_k_indices = np.argsort(skill_similarities)[-10:] # Get top 10\n",
        "\n",
        "    found_perfect_match = False\n",
        "    best_skill_job_id = top_k_indices[-1] # Fallback\n",
        "\n",
        "    for job_id in reversed(top_k_indices):\n",
        "        job = jobs_df.iloc[job_id]\n",
        "        exp_match = user['experience'] >= job['experience_required']\n",
        "        sal_match = (user['salary_expectation'] >= job['salary_min']) and \\\n",
        "                    (user['salary_expectation'] <= job['salary_max'])\n",
        "\n",
        "        if exp_match and sal_match:\n",
        "            interaction_data_pos.append({'user_id': user_id, 'job_id': job_id, 'shortlisted': 1})\n",
        "            found_perfect_match = True\n",
        "            break\n",
        "\n",
        "    if not found_perfect_match:\n",
        "        interaction_data_pos.append({'user_id': user_id, 'job_id': best_skill_job_id, 'shortlisted': 1})\n",
        "\n",
        "# --- Generate negative samples (random/bad matches) ---\n",
        "print(f\"Generating {NUM_INTERACTIONS // 2} negative samples...\")\n",
        "for _ in range(NUM_INTERACTIONS // 2):\n",
        "    user_id = np.random.randint(0, NUM_USERS)\n",
        "    job_id = np.random.randint(0, NUM_JOBS)\n",
        "\n",
        "    user = users_df.iloc[user_id]\n",
        "    job = jobs_df.iloc[job_id]\n",
        "\n",
        "    if user['experience'] < job['experience_required']:\n",
        "        shortlisted = 0\n",
        "    elif user['salary_expectation'] > job['salary_max'] + 20000:\n",
        "        shortlisted = 0\n",
        "    else:\n",
        "        shortlisted = 0\n",
        "\n",
        "    interaction_data_neg.append({'user_id': user_id, 'job_id': job_id, 'shortlisted': shortlisted})\n",
        "\n",
        "# --- NEW: Create a balanced 40/60 dataset ---\n",
        "print(\"Balancing and cleaning dataset for 40/60 split...\")\n",
        "\n",
        "# Convert to DataFrames and drop duplicates from each list\n",
        "pos_df = pd.DataFrame(interaction_data_pos).drop_duplicates(subset=['user_id', 'job_id'])\n",
        "neg_df = pd.DataFrame(interaction_data_neg).drop_duplicates(subset=['user_id', 'job_id'])\n",
        "\n",
        "# Make sure no positive samples are in the negative list\n",
        "neg_df = neg_df.merge(pos_df[['user_id', 'job_id']], on=['user_id', 'job_id'], how='left', indicator=True)\n",
        "neg_df = neg_df[neg_df['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
        "\n",
        "# We have clean pools. Now, let's balance to 40% positive (0.4) and 60% negative (0.6).\n",
        "# Ratio is 0.4 / 0.6 = 2/3. We need 2 positive samples for every 3 negative ones.\n",
        "pos_clean_count = len(pos_df)\n",
        "neg_clean_count = len(neg_df)\n",
        "print(f\"Cleaned samples: {pos_clean_count} positive, {neg_clean_count} negative.\")\n",
        "\n",
        "# Let's find the limiting factor.\n",
        "# We want N_pos = (2/3) * N_neg\n",
        "\n",
        "# Case 1: Positives are the bottleneck.\n",
        "# We use all available positives and find how many negatives we need.\n",
        "required_neg_count = int(pos_clean_count * (0.6 / 0.4)) # or 1.5 * pos_clean_count\n",
        "\n",
        "if neg_clean_count >= required_neg_count:\n",
        "    print(f\"Using all {pos_clean_count} positive samples, downsampling negatives...\")\n",
        "    final_pos_df = pos_df\n",
        "    final_neg_df = neg_df.sample(required_neg_count)\n",
        "else:\n",
        "    # Case 2: Negatives are the bottleneck.\n",
        "    # We use all available negatives and find how many positives we need.\n",
        "    print(f\"Using all {neg_clean_count} negative samples, downsampling positives...\")\n",
        "    required_pos_count = int(neg_clean_count * (0.4 / 0.6)) # or (2/3) * neg_clean_count\n",
        "\n",
        "    final_neg_df = neg_df\n",
        "    final_pos_df = pos_df.sample(required_pos_count)\n",
        "\n",
        "# Combine and shuffle\n",
        "interactions_df = pd.concat([final_pos_df, final_neg_df]).sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Save the final file\n",
        "interactions_df.to_csv('interactions.csv', index=False)\n",
        "\n",
        "print(\"interactions.csv created.\")\n",
        "print(f\"Total interactions: {len(interactions_df)}\")\n",
        "print(f\"Positive interactions: {interactions_df['shortlisted'].sum()} ({(interactions_df['shortlisted'].sum() / len(interactions_df) * 100):.1f}%)\")"
      ]
    }
  ]
}